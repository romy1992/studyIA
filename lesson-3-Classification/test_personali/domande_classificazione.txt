1. Qual è l'obiettivo principale di un modello di classificazione?
a) Predire valori continui
b) Dividere i dati in cluster
c) Prevedere categorie o classi
d) Ridurre il rumore nei dati
2. Quale algoritmo è comunemente utilizzato per la classificazione?
a) Regressione lineare
b) K-Nearest Neighbors (KNN)
c) Regressione Ridge
d) PCA
3. Cosa rappresenta la curva ROC in classificazione?
a) La precisione del modello
b) La relazione tra sensibilità e specificità
c) Il valore di R²
d) La varianza del modello
4. Qual è una metrica comune per valutare i modelli di classificazione?
a) Mean Squared Error (MSE)
b) Area sotto la curva ROC (AUC)
c) Variance Inflation Factor (VIF)
d) Log-loss
5. Cosa rappresenta il termine 'accuracy'?
a) La frazione di predizioni corrette sul totale
b) La capacità del modello di generalizzare su nuovi dati
c) L'errore medio assoluto
d) La somma degli errori al quadrato
6. Quale tecnica è utile per gestire un dataset sbilanciato?
a) Sottocampionamento della classe maggioritaria
b) Oversampling della classe minoritaria
c) Utilizzo di metriche come F1-score
d) Tutte le precedenti
7. Cosa significa 'overfitting' in un modello di classificazione?
a) Adattarsi troppo ai dati di training, perdendo capacità di generalizzazione
b) Non adattarsi ai dati di training
c) Generalizzare perfettamente su nuovi dati
d) Ridurre la complessità del modello
8. Quale funzione di attivazione è comunemente usata nella classificazione binaria?
a) ReLU
b) Sigmoid
c) Tanh
d) Softmax
9. Cosa rappresenta il termine 'precision'?
a) La capacità del modello di catturare tutte le istanze rilevanti
b) La proporzione di predizioni corrette per una classe specifica
c) La sensibilità del modello
d) La generalizzazione del modello
10. Quale metodo è utile per valutare le performance su dataset sbilanciati?
a) Cross-validation
b) Matrice di confusione
c) F1-score
d) R²
11. Cos'è il metodo Naive Bayes?
a) Un algoritmo di regressione
b) Un algoritmo basato sul teorema di Bayes
c) Un metodo per la riduzione della dimensionalità
d) Un metodo non supervisionato
12. Qual è il principio base del K-Nearest Neighbors?
a) Minimizzare l'errore quadratico
b) Classificare in base alla maggioranza dei vicini più vicini
c) Utilizzare kernel per modellare relazioni non lineari
d) Ridurre la dimensionalità
13. Qual è una caratteristica dell'algoritmo SVM?
a) Utilizza la matrice di confusione per la classificazione
b) Cerca di massimizzare il margine tra le classi
c) Utilizza solo feature categoriche
d) Minimizza l'errore logaritmico
14. Quale delle seguenti tecniche è utile per migliorare i modelli di classificazione?
a) Normalizzazione delle feature
b) Cross-validation
c) Hyperparameter tuning
d) Tutte le precedenti
15. Quando si utilizza il softmax?
a) Per la classificazione binaria
b) Per la classificazione multiclasse
c) Per normalizzare feature numeriche
d) Per ottimizzare un modello di regressione
16. Qual è una limitazione dell'algoritmo KNN?
a) Non può essere utilizzato con dati categorici
b) Richiede un'ampia quantità di memoria
c) Non supporta la classificazione binaria
d) Non può gestire outlier
17. Quale tecnica è utilizzata per bilanciare un dataset sbilanciato?
a) Feature scaling
b) Algoritmi ensemble
c) Synthetic Minority Oversampling Technique (SMOTE)
d) Eliminazione dei dati mancanti
18. Cosa rappresenta il termine 'recall'?
a) La proporzione di istanze rilevanti catturate dal modello
b) La precisione del modello per una classe specifica
c) La sensibilità moltiplicata per la specificità
d) La differenza tra i valori predetti e osservati
19. Qual è il vantaggio dell'algoritmo Random Forest?
a) Non richiede normalizzazione delle feature
b) È robusto agli outlier
c) Riduce l'overfitting rispetto a un singolo albero
d) Tutte le precedenti
20. Quando si utilizza la regularizzazione L1 in classificazione?
a) Per ridurre la dimensionalità selezionando feature rilevanti
b) Per ridurre il bias del modello
c) Per ottimizzare la funzione di perdita
c) Per aumentare la complessità del modello
21. Qual è lo scopo dell'algoritmo Decision Tree?
a) Creare regole gerarchiche per la classificazione
b) Ottimizzare il valore di R²
c) Ridurre la dimensionalità del dataset
d) Identificare outlier
22. Quale approccio ensemble combina la media ponderata delle predizioni?
a) Random Forest
b) Bagging
c) Boosting
d) Stacking
23. Qual è l'obiettivo del gradient boosting in classificazione?
a) Ridurre il numero di feature
b) Utilizzare un kernel per separare classi
c) Ottimizzare successivamente modelli deboli per migliorare l'accuratezza
d) Minimizzare l'errore logaritmico
24. Cosa indica un valore di F1-score pari a 1?
a) Il modello ha una precisione perfetta
b) Il modello ha una recall perfetta
c) Il modello ha una perfetta combinazione di precision e recall
d) Il modello non ha fatto errori
25. Quale algoritmo è particolarmente utile per grandi dataset con molte feature?
a) Logistic Regression
b) Support Vector Machines (SVM)
c) Naive Bayes
d) Random Forest
26. Quando è utile applicare la tecnica di feature scaling?
a) Quando si utilizzano algoritmi basati sulla distanza, come KNN
b) Quando si addestrano modelli ensemble
c) Quando il dataset è bilanciato
d) Quando si utilizza Naive Bayes
27. Cosa rappresenta il termine 'specificità' in classificazione?
a) La capacità del modello di identificare correttamente le istanze negative
b) La proporzione di predizioni corrette per una classe specifica
c) La sensibilità del modello
d) La capacità di ridurre il bias
28. Quale tecnica è utile per gestire dati mancanti prima di una classificazione?
a) Eliminazione dei record
b) Imputazione con valori medi o modali
c) Utilizzo di un modello di predizione per i dati mancanti
d) Tutte le precedenti
29. Qual è il vantaggio del metodo ensemble in classificazione?
a) Migliora la robustezza del modello
b) Riduce la varianza
c) Migliora la generalizzazione su nuovi dati
d) Tutte le precedenti
30. Quale funzione di perdita è utilizzata per la classificazione binaria?
a) Mean Squared Error (MSE)
b) Hinge loss
c) Log-loss
d) Cross-entropy loss
11. Cos'è il metodo Naive Bayes?
a) Un algoritmo di regressione
b) Un algoritmo basato sul teorema di Bayes
c) Un metodo per la riduzione della dimensionalità
d) Un metodo non supervisionato
12. Qual è il principio base del K-Nearest Neighbors?
a) Minimizzare l'errore quadratico
b) Classificare in base alla maggioranza dei vicini più vicini
c) Utilizzare kernel per modellare relazioni non lineari
d) Ridurre la dimensionalità
13. Qual è una caratteristica dell'algoritmo SVM?
a) Utilizza la matrice di confusione per la classificazione
b) Cerca di massimizzare il margine tra le classi
c) Utilizza solo feature categoriche
d) Minimizza l'errore logaritmico
14. Quale delle seguenti tecniche è utile per migliorare i modelli di classificazione?
a) Normalizzazione delle feature
b) Cross-validation
c) Hyperparameter tuning
d) Tutte le precedenti
15. Quando si utilizza il softmax?
a) Per la classificazione binaria
b) Per la classificazione multiclasse
c) Per normalizzare feature numeriche
d) Per ottimizzare un modello di regressione
16. Qual è una limitazione dell'algoritmo KNN?
a) Non può essere utilizzato con dati categorici
b) Richiede un'ampia quantità di memoria
c) Non supporta la classificazione binaria
d) Non può gestire outlier
17. Quale tecnica è utilizzata per bilanciare un dataset sbilanciato?
a) Feature scaling
b) Algoritmi ensemble
c) Synthetic Minority Oversampling Technique (SMOTE)
d) Eliminazione dei dati mancanti
18. Cosa rappresenta il termine 'recall'?
a) La proporzione di istanze rilevanti catturate dal modello
b) La precisione del modello per una classe specifica
c) La sensibilità moltiplicata per la specificità
d) La differenza tra i valori predetti e osservati
19. Qual è il vantaggio dell'algoritmo Random Forest?
a) Non richiede normalizzazione delle feature
b) È robusto agli outlier
c) Riduce l'overfitting rispetto a un singolo albero
d) Tutte le precedenti
20. Quando si utilizza la regularizzazione L1 in classificazione?
a) Per ridurre la dimensionalità selezionando feature rilevanti
b) Per ridurre il bias del modello
c) Per ottimizzare la funzione di perdita
c) Per aumentare la complessità del modello
21. Qual è lo scopo dell'algoritmo Decision Tree?
a) Creare regole gerarchiche per la classificazione
b) Ottimizzare il valore di R²
c) Ridurre la dimensionalità del dataset
d) Identificare outlier
22. Quale approccio ensemble combina la media ponderata delle predizioni?
a) Random Forest
b) Bagging
c) Boosting
d) Stacking
23. Qual è l'obiettivo del gradient boosting in classificazione?
a) Ridurre il numero di feature
b) Utilizzare un kernel per separare classi
c) Ottimizzare successivamente modelli deboli per migliorare l'accuratezza
d) Minimizzare l'errore logaritmico
24. Cosa indica un valore di F1-score pari a 1?
a) Il modello ha una precisione perfetta
b) Il modello ha una recall perfetta
c) Il modello ha una perfetta combinazione di precision e recall
d) Il modello non ha fatto errori
25. Quale algoritmo è particolarmente utile per grandi dataset con molte feature?
a) Logistic Regression
b) Support Vector Machines (SVM)
c) Naive Bayes
d) Random Forest
26. Quando è utile applicare la tecnica di feature scaling?
a) Quando si utilizzano algoritmi basati sulla distanza, come KNN
b) Quando si addestrano modelli ensemble
c) Quando il dataset è bilanciato
d) Quando si utilizza Naive Bayes
27. Cosa rappresenta il termine 'specificità' in classificazione?
a) La capacità del modello di identificare correttamente le istanze negative
b) La proporzione di predizioni corrette per una classe specifica
c) La sensibilità del modello
d) La capacità di ridurre il bias
28. Quale tecnica è utile per gestire dati mancanti prima di una classificazione?
a) Eliminazione dei record
b) Imputazione con valori medi o modali
c) Utilizzo di un modello di predizione per i dati mancanti
d) Tutte le precedenti
29. Qual è il vantaggio del metodo ensemble in classificazione?
a) Migliora la robustezza del modello
b) Riduce la varianza
c) Migliora la generalizzazione su nuovi dati
d) Tutte le precedenti
30. Quale funzione di perdita è utilizzata per la classificazione binaria?
a) Mean Squared Error (MSE)
b) Hinge loss
c) Log-loss
d) Cross-entropy loss
